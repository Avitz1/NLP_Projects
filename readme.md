<!---A repository for projects related to Natural Language Processing--->
# NLP Projects
This repository contains projects related to Natural Language Processing. The projects are implemented in Python and R. The projects are as follows:
1. Sentiment Analysis (Text Classification)
2. My_Semantle
3. Text Generation
4. Probabilistic Language Models
5. Tokenization

## Sentiment Analysis
Sentiment analysis is the process of determining whether a piece of writing is positive, negative or neutral. It’s also known as opinion mining, deriving the opinion or attitude of a speaker. A common use case of sentiment analysis is to discover how people feel about a particular topic. Sentiment analysis is widely used by businesses to understand the sentiment of their brand, product or service while also monitoring what people are saying about them in social media. Sentiment analysis is also used by researchers to understand the general sentiment of the public towards various topics.
In this project, we used Kaggle dataset of women’s clothing reviews to perform sentiment analysis. The dataset contains 23486 rows and 10 feature variables. Each row corresponds to a customer review.

## My_Semantle
An implementation of the Semantle game in which a user has to guess a word the model has randomly chosen. The model uses a word embedding to find the most similar words to the word the user has to guess. The user has to guess the word by choosing the most similar word from a list of 4 words. The user gets 1 point for each correct guess and loses 1 point for each incorrect guess. The game ends when the user has guessed the word or when the user has 5 incorrect guesses. The user can choose to play again or quit the game.
In each round the model informs the user how close he is to the word he has to guess. It is done by using word embeddings in which words with semantic proximity will have "close" embeddings, unlike in the language itself (where words with semantic proximity may be far apart in the vocabulary).

## Text Generation and N-grams
An N-gram is a language model that predicts the next word in a sentence based on the previous n words. In this project, we experimented with unigrams, bigrams and trigrams. We used the Brown corpus to train the models. The Brown corpus is a collection of 500 texts from the Brown University. The texts are categorized into 15 genres. The corpus contains 1,161,192 words and 57,340 sentences. The corpus is available in the NLTK library.

## Probabilistic sentence derivation

Given a Context-free probabilistic grammar (CFG) and a sentence, we'd like to understand whether the sentence "belongs" to the language generated by the CFG, and if it does, find the most probable derivation.
For example, in the English language (not a CFG..), the sentence "He saw the man with the telescope" could have 2 syntactic derivations:
1. "He saw the man using a telescope"
2. "He saw the man that was holding a telescope"

In this case it's hard to determine a-priory what was the meaning, but it helps to understand that the "roles" in sentences are not fixed, and that the same sentence can be interpreted in different ways. 

We implemented a probabilistic version of the CKY algorithm to find the most probable derivation of a sentence given a CFG. 

## Tokenization

A tokenizer was implemented to tokenize a given text. 
Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. The list of tokens becomes input for further processing such as parsing or text mining.
A piece of text like "I haven't been to the USA in the last 1.5 years, have you?" can be tokenized into the following tokens:   
["I", "haven't", "been", "to", "the", "USA", "in", "the", "last", "1.5", "years", ",", "have", "you", "?"]

This process is useful for many NLP tasks such as part-of-speech tagging, named entity recognition, and machine translation, because a machine can only understand numbers, not words or sentences in their natural form.

